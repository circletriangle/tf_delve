{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.python.keras import backend as K\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "import importlib\n",
    "\n",
    "\n",
    "import Clone_Model\n",
    "import rsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./cloning_eager_dis/checkpoints/cloning/default_checkpoint.ckpt\"\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True, verbose=1)\n",
    "\n",
    "log_dir_clone = \"./cloning_eager_dis/logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir_clone, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3 steps, validate on 1 steps\n",
      "3/3 [==============================] - 0s 23ms/step - loss: 3.1866 - val_loss: 1.7910\n",
      "RESET Weights!\n",
      "RESET Weights!\n",
      "RESET Weights!\n",
      "RESET Weights!\n",
      "RESET Weights!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected model_input to have 2 dimensions, but got array with shape (1, 28, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-63fc8cab4945>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mclone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClone_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msatify_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m clone.compile(optimizer=keras.optimizers.Adam(),\n",
      "\u001b[0;32m~/Desktop/Thesis/New_Repo/Clone_Model.py\u001b[0m in \u001b[0;36msatify_model\u001b[0;34m(model, compile_dict)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m#z_in = np.ones(shape=model.layers[0].input_shape[-2:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0mz_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Wrong: predictions don't match original\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    705\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_or_infer_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m     x, _, _ = model._standardize_user_data(\n\u001b[0;32m--> 707\u001b[0;31m         x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m    708\u001b[0m     return predict_loop(\n\u001b[1;32m    709\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2306\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2307\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2308\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2333\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2335\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2337\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf/lib/python3.7/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    571\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected model_input to have 2 dimensions, but got array with shape (1, 28, 28)"
     ]
    }
   ],
   "source": [
    "#      CLONE MODEL EAGER DISABLED\n",
    "###########################################\n",
    "importlib.reload(rsc)\n",
    "importlib.reload(Clone_Model)\n",
    "\n",
    "K.clear_session() \n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "batch_size = None #doesnt work/conflict tensorboard_callback\n",
    "train, test = rsc.get_titanic_dataset()\n",
    "\n",
    "\n",
    "\n",
    "model = rsc.get_model(None)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=keras.losses.BinaryCrossentropy(),\n",
    "    run_eagerly = tf.executing_eagerly()\n",
    "    )\n",
    "\n",
    "\n",
    "tensorboard_callback.set_model(model)\n",
    "\n",
    "history = model.fit(train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=3, \n",
    "                    validation_data=test, \n",
    "                    validation_steps=1, \n",
    "                    callbacks=[])\n",
    "\n",
    "\n",
    "clone = Clone_Model.satify_model(model)\n",
    "\n",
    "clone.compile(optimizer=keras.optimizers.Adam(),\n",
    "              loss=keras.losses.BinaryCrossentropy(),\n",
    "              run_eagerly = tf.executing_eagerly())\n",
    "\n",
    "sat_cb = Clone_Model.sat_results()\n",
    "\n",
    "\n",
    "history = clone.fit(train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=3,\n",
    "                    validation_data=test, \n",
    "                    validation_steps=1, \n",
    "                    callbacks=[sat_cb])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 3s 57us/sample - loss: 2.0756 - accuracy: 0.8500 - val_loss: 0.4505 - val_accuracy: 0.8936\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.3748 - accuracy: 0.9089 - val_loss: 0.3705 - val_accuracy: 0.9128\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 58us/sample - loss: 0.3050 - accuracy: 0.9260 - val_loss: 0.3273 - val_accuracy: 0.9220\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 53us/sample - loss: 0.2741 - accuracy: 0.9340 - val_loss: 0.3798 - val_accuracy: 0.9065\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.2543 - accuracy: 0.9387 - val_loss: 0.2959 - val_accuracy: 0.9369\n"
     ]
    }
   ],
   "source": [
    "#      CLASSIFY MNIST EAGER DISABLED\n",
    "###########################################\n",
    "importlib.reload(rsc)\n",
    "importlib.reload(Clone_Model)\n",
    "\n",
    "K.clear_session() \n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "batch_size = 20 \n",
    "\n",
    "train, test = rsc.get_mnist()\n",
    "\n",
    "model = rsc.get_model_unitlist(hidden_layers_spec=[128])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    "    run_eagerly = tf.executing_eagerly()\n",
    "    )\n",
    "\n",
    "\n",
    "tensorboard_callback.set_model(model)\n",
    "\n",
    "history = model.fit(train[0],\n",
    "                    train[1],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=5, \n",
    "                    steps_per_epoch=None, \n",
    "                    validation_data=test, \n",
    "                    validation_steps=1, \n",
    "                    callbacks=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.layers.core.Flatten'>\n",
      "ListWrapper(['o_s', 'r_s', 's_s'])\n",
      "ListWrapper(['o_s', 'r_s', 's_s'])\n",
      "RESET Weights!\n",
      "RESET Weights!\n",
      "Clone_Model.satify_model() end.\n",
      "WARNING:tensorflow:When passing input data as arrays, do not specify `steps_per_epoch`/`steps` argument. Please use `batch_size` instead.\n",
      "Train on 3 samples, validate on 10000 samples\n",
      "AGGREG UPDATESTATE() CALLED: 0.0\n",
      "AGGREG UPDATESTATE() CALLED: 0.0\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(128, 128), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(128,), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(10, 10), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(10,), dtype=float64)\n",
      "\n",
      "Logs from batch callback: {'batch': 0, 'size': 1, 'loss': 0.22063896, 'accuracy': 0.9450167}\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.2206 - accuracy: 0.9450AGGREG UPDATESTATE() CALLED: 0.0\n",
      "AGGREG UPDATESTATE() CALLED: 0.0\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(128, 128), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(128,), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(10, 10), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(10,), dtype=float64)\n",
      "\n",
      "Logs from batch callback: {'batch': 1, 'size': 1, 'loss': 0.18453525, 'accuracy': 0.94851667}\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.2026 - accuracy: 0.9485AGGREG UPDATESTATE() CALLED: 0.0\n",
      "AGGREG UPDATESTATE() CALLED: 0.0\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(128, 128), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(128,), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(10, 10), dtype=float64)\n",
      "AGGREG UPDATESTATE() CALLED: Tensor(\"update_tensor:0\", shape=(10,), dtype=float64)\n",
      "\n",
      "Logs from batch callback: {'batch': 2, 'size': 1, 'loss': 0.16922633, 'accuracy': 0.9508833}\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.1915 - accuracy: 0.9509\n",
      "Logs from epoch callback: {'loss': 0.19146684805552164, 'accuracy': 0.9508833, 'val_loss': 0.24173198640346527, 'val_accuracy': 0.947}\n",
      "RESET Weights!\n",
      "\n",
      "Logs from epoch callback: {'loss': 0.19146684805552164, 'accuracy': 0.9508833, 'val_loss': 0.24173198640346527, 'val_accuracy': 0.947}\n",
      "RESET Weights!\n",
      "3/3 [==============================] - 3s 1s/step - loss: 0.1915 - accuracy: 0.9509 - val_loss: 0.2417 - val_accuracy: 0.9470\n"
     ]
    }
   ],
   "source": [
    "#      CLONE MNIST MODEL EAGER DISABLED\n",
    "###########################################\n",
    "importlib.reload(rsc)\n",
    "importlib.reload(Clone_Model)\n",
    "#K.clear_session() \n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "clone = Clone_Model.satify_model(model)\n",
    "\n",
    "sat_cb = Clone_Model.sat_results()\n",
    "\n",
    "history = clone.fit(train[0],\n",
    "                    train[1],\n",
    "                    batch_size=3,\n",
    "                    epochs=1,\n",
    "                    steps_per_epoch=3,\n",
    "                    validation_data=test, \n",
    "                    validation_steps=1, \n",
    "                    callbacks=[sat_cb])\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n",
      "{'validation_data': None, 'model': <tensorflow.python.keras.engine.sequential.Sequential object at 0x7fe02832fc90>, '_chief_worker_only': None, 'history': {'loss': [0.19146684805552164], 'accuracy': [0.9508833], 'val_loss': [0.24173198640346527], 'val_accuracy': [0.947]}, 'params': {'batch_size': 3, 'epochs': 1, 'steps': 3, 'samples': 3, 'verbose': 1, 'do_validation': True, 'metrics': ['loss', 'accuracy', 'o_sdense', 'o_sdense_1', 'val_loss', 'val_accuracy', 'val_o_sdense', 'val_o_sdense_1']}, 'epoch': [0]}\n"
     ]
    }
   ],
   "source": [
    "print(history.history.keys())\n",
    "print(history.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Warum macht tf probleme und zeigt auf auskommentierten code?\n",
    "        #Default to k=1 if first EVal explains delta of variance alone \n",
    "        #TODO change condition so graph mode works without tf.function\n",
    "        #if not \n",
    "        #    #cond = tf.constant([True], dtype=tf.bool)\n",
    "            #print(\"cond with default catch: {}\".format(cond))   \n",
    "        \"\"\"    \n",
    "        \n",
    "    \n",
    "    #k = max(len(tf.where(cond)), 1 ) #TODO use min(1,tf.where()) instead of checking above\n",
    "        \n",
    "        #print(\"tf.where(): {}\".format(tf.where(cond)))\n",
    "        #print(\"K: in ll_fun {}\".format(k))\n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf] *",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
